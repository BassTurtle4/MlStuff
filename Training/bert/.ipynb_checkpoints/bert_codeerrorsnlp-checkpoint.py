# -*- coding: utf-8 -*-
"""Doc2Vec CodeErrorsNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B4BWUF_ZXMikNXm7WtBdaZ_774tp6Vxp
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import math
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
# %matplotlib inline

from gensim.models import Word2Vec
from matplotlib.ticker import MaxNLocator

#### wordcloud
MAX_WORDS = 20
COLLOCATION = 1 #5
COLLOCATIONS = False

#from google.colab import drive
#drive.mount('/gdrive')

DATA = "../Errors_data.csv"

"""## Loading the data"""

df = pd.read_csv(DATA,header=None, names=['Error', 'Category','x','y'])
df = df.drop(['x','y'], axis=1)
df.head(10)

df.shape

print("Total unique categories:", df['Category'].nunique())

math.sqrt(df['Category'].nunique())

df.iloc[182]['Error']

# df['Ctegory'].value_counts().plot.bar()

ax = df['Category'].value_counts().head(5).plot.bar()

ax.set_title('Top-5 Categories')
ax.set_xlabel('Category')
ax.set_ylabel('Count')

#plt.show()

# Remove whitespace around plot
fig = ax.get_figure()
fig.tight_layout()

# Save just the plot area to PDF
fig.savefig('./top5_category.pdf', bbox_inches='tight')

df['Category'].value_counts().head(5).plot.bar()

df['Category'].value_counts().tail(5).plot.bar()

# df['Ctegory'].value_counts().plot.bar()

ax = df['Error'].value_counts().head(5).plot.bar()

ax.set_title('Top-5 Errors')
ax.set_xlabel('Error')
ax.set_ylabel('Count')

#plt.show()

# Remove whitespace around plot
fig = ax.get_figure()
# fig.tight_layout()

# Save just the plot area to PDF
fig.savefig('./top5_error.pdf', bbox_inches='tight')

"""## Preprocessing the data"""

def preprocessing(text):

    # text = text.replace("/home/imhmede/workspace/hello.java:", "")
    # text = text.split()
    # text = [txt for i, txt in enumerate(text) if i!= 1]
    # print(text)
    # text = " ".join(text)
    # text = change_construct(text)
    # text = re.sub(r'[^\w\s\<\>]', '', text)
    # text = [txt.strip() for txt in text.split() if not txt.isdigit()]
    # text = " ".join(text)
    # text = text.replace('_', '')
    # text = text.lower()
    # text = text.replace("error", "<ERROR>")
    # text = text.replace("warn", "<WARN>")
    # # txt = re.sub(' +', ' ', text)
    # text = text.split()
    # text = " ".join(text)
    text = change_name_next(text)
    # text = change_def_next(text)
    text = change_class_next(text)
    text = change_method_next(text)
    text = change_brackets(text)
    text = change_isnot(text)
    text = change_construct(text)
    text = change_use_brackets(text)
    text = change_shouldbe(text)
    text = change_parameter_next(text)

    text = text.lower()
    return text

def change_name_next(text):
    l = text.split()
    j = ['<A_NAME>' if i>0 and l[i-1] in ["Name"] else l[i] for i in range(len(l))]
    return ' '.join(j)

def change_def_next(text):
    l = text.split()
    j = ['<A_FUNC_NAME>' if i>0 and l[i-1] in ["def"] else l[i] for i in range(len(l))]
    return ' '.join(j)

def change_class_next(text):
    l = text.split()
    j = ['<A_CLASS_NAME>' if i>0 and l[i-1] in ["Class"] else l[i] for i in range(len(l))]
    return ' '.join(j)

def change_parameter_next(text):
    l = text.split()
    j = ['<A_PARAMETER_NAME>' if i>0 and l[i-1] in ["Parameter"] else l[i] for i in range(len(l))]
    return ' '.join(j)

def change_method_next(text):
    l = text.split()
    j = ['<A_METHOD_NAME>' if i>0 and l[i-1] in ["method"] else l[i] for i in range(len(l))]
    return ' '.join(j)

def change_brackets(text):
    return re.sub(r"'[{()}]'", "<BRAKET>", text)

def change_construct(text):
    return re.sub(r"'[a-zA-Z]+'", "<CONSTRUCT>", text)

def change_isnot(text):
  if " is not" in text:
    new_text = re.sub(r'(^.*) is not', r'<OPERATOR> is not', text)
  else:
    new_text = text

  return new_text

def change_shouldbe(text):
  if " should be on" in text:
    new_text = re.sub(r'(^.*) should be on', r'<OPERATOR> should be on', text)
  else:
    new_text = text

  return new_text

def change_construct(text):
  if " construct must use" in text:
    new_text = re.sub(r'(^.*) construct must use', r'<OPERATOR> construct must use', text)
  else:
    new_text = text

  return new_text

def change_use_brackets(text):
  if "'{}'s" in text:
    new_text = re.sub(r"'{}'s", r"<BRACKET>", text)
  else:
    new_text = text

  return new_text

# n_text = preprocessing(content_list[160])
# print(n_text)

# id_ = 1002
id_ = 103
print(df.iloc[id_]['Error'])

n_text = preprocessing(df.iloc[id_]['Error'])
print(n_text)

"""## Create a dataframe"""

df['clean'] = df['Error'].apply(preprocessing)

df.head()

df.iloc[1]["clean"]

df.iloc[114]["clean"]

df.iloc[114]["clean"]

"""## Create a Word Cloud"""

all_text = []
def get_all_text(text):
    all_text.append(text)

df["clean"].apply(get_all_text)
text = "\n".join(all_text)
# print(text)

# wordcloud = WordCloud(background_color="white").generate(text)

#Create the wordcloud object
wordcloud = WordCloud(collocations= False,
                        background_color="white",
                        collocation_threshold = 100,
                        max_words = 10,
                        relative_scaling = 1).generate(text)
#plot
plt.figure(figsize=(7,5), dpi=100)
plt.imshow(wordcloud, interpolation='bilInear')
plt.axis('off')
plt.title("Word Cloud for Error Descriptions")
plt.savefig("./wordcloud.pdf", bbox_inches='tight')
#plt.show()

# Process text
frequencies = wordcloud.process_text(text)

print(frequencies)

"""## Using Doc2Vec"""

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

DIMENSIONS = 64
WINDOW = 5

all_texts = df['clean'].tolist()
all_texts = [text.split() for text in all_texts]
print(len(all_texts))

print(all_texts[:2])
print(all_texts[-2:])

df["textList"] = all_texts

df

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_texts)]

"""### Get embeddings for multiple dimensions"""
import torch
from transformers import BertTokenizer, BertModel

print("Cuda??? " + str(torch.cuda.is_available()))

if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("Using GPU:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print("Using CPU")

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = BertModel.from_pretrained(model_name).to(device)

batch_size = 256

dims = [16, 32, 64, 128, 256, 512]

text_documents = [doc.words for doc in documents]
all_embeddings = []
df = pd.DataFrame(index=range(len(text_documents)))

def get_bert_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    inputs.to(device)  # Move the inputs to the GPU
    with torch.no_grad():
        outputs = model(**inputs)
    #print(type(outputs))
    return outputs.last_hidden_state.cpu().numpy()  # Move the result back to CPU

dims = [16]#,32,64,128,256

for dim in dims:
    print(f"BERT embeddings for {dim} dimensions")
    col_name = f'embedding_{dim}'
    df[col_name] = df['clean'].apply(get_bert_embeddings)
#ADD DENSE LAYER
#    OR
#Make it work with the bert
df.head()

'''
dims = [16, 32, 64, 128, 256, 512]

for dim in dims:
    print(f"Doc2Vec for {dim} dimensions")
    # Init model
    doc2_vec_model = Doc2Vec(documents, vector_size=dim, epochs=20, window=WINDOW, min_count=5, seed=0)

    # Get vectors
    all_d2v_vecs = [doc2_vec_model.dv[i] for i in range(len(doc2_vec_model.dv))]

    # Add to df
    col_name = 'embedding_' + str(dim)
    df[col_name] = all_d2v_vecs
df.head()
'''

"""### Get the explained variance"""

from sklearn.decomposition import PCA

# PCA analysis
variance_explained = []
for dim in dims:
    col = 'embedding_'+str(dim)
    print(col)
    embeddings=df[col].tolist()
    pca = PCA(n_components=2)
    pca.fit(embeddings)
    variance_explained.append(pca.explained_variance_ratio_.sum().round(2))

print(variance_explained)

# Plot variance explained
import matplotlib.pyplot as plt
plt.plot(dims, variance_explained)
# Set x ticks
plt.xticks(dims,rotation=90)

plt.title('Variance Explained vs Dimensionality')
plt.xlabel('Dimensions')
plt.ylabel('Variance Explained')
plt.savefig("./embeddings_variance.pdf", bbox_inches='tight')
#plt.show()

"""Best Doc2Vec dimension seems to be 128

## Let's find the best k for clustering
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score

# Range of k to check
k_range = range(2, 20)

sil_scores = []
cal_scores = []

for k in k_range:
  # Cluster data into k clusters
  kmeans = KMeans(n_clusters=k)
  clusters = kmeans.fit_predict(df['embedding_128'].tolist())

  # Score with metrics
  sil_score = silhouette_score(df['embedding_128'].tolist(), clusters)
  sil_scores.append(sil_score)

  cal_score = calinski_harabasz_score(df['embedding_128'].tolist(), clusters)
  cal_scores.append(cal_score)

# Plot scores
plt.plot(k_range, sil_scores, label='Silhouette')
new_list = range(math.floor(min(k_range)), math.ceil(max(k_range))+1)
plt.xticks(new_list)
plt.title('Silhouette Cluster Evaluation Scores')
plt.xlabel('Size of k')
plt.ylabel('Score')
#plt.legend()
plt.savefig("./shilouette_k.pdf", bbox_inches='tight')
#plt.show()

print(sil_scores)

# Plot scores
plt.plot(k_range, cal_scores, label='Calinski-Harabasz')
new_list = range(math.floor(min(k_range)), math.ceil(max(k_range))+1)
plt.xticks(new_list)
plt.title('Cluster Evaluation Scores')
plt.xlabel('k')
plt.legend()
#plt.show()

"""It looks that k=7 is fine

### Lets use the 128 dimensions and 7 clusters
"""

all_d2v_vecs = df['embedding_128'].tolist()

np.shape(all_d2v_vecs)

all_d2v_vecs[0]

kmeans = KMeans(n_clusters=7, random_state=0).fit(all_d2v_vecs)

kmeans.labels_

df["d2v_clusters"] = kmeans.labels_
df

ax = df['d2v_clusters'].value_counts().plot.bar()

ax.set_title('Distribution of Clusters')
ax.set_xlabel('Clusters')
ax.set_ylabel('Number of Errors')

#plt.show()

# Remove whitespace around plot
fig = ax.get_figure()
# fig.tight_layout()

# Save just the plot area to PDF
fig.savefig('./hist_clusters.pdf', bbox_inches='tight')

clust_number = 2
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

clust_number = 0
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

clust_number = 5
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

clust_number = 1
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

clust_number = 6
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

clust_number = 3
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

clust_number = 4
print(df[df['d2v_clusters'] == clust_number]['clean'])
print(df[df['d2v_clusters'] == clust_number]['clean'].count())

"""### Silhouette Score"""

from yellowbrick.cluster import SilhouetteVisualizer


# Assuming 'embedding128' is your pandas Series containing embeddings
# Convert the Series to a list of lists
embedding_list = df['embedding_128'].tolist()
embedding_list_of_lists = np.array([embedding.tolist() for embedding in embedding_list])

# Create a SilhouetteVisualizer instance
title = "Silhouette Plot of k-means Clustering"
visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick', title=title)


# Fit the visualizer with the standardized embedding data
visualizer.fit(embedding_list_of_lists)
# Export the plot to a PDF with a tight layout
visualizer.show(outpath="./silhouette_plot.pdf")

# Display the visualization
visualizer.poof()

"""### Let's use UMAP to plot the clusters

"""

data_embeds = pd.DataFrame()
data_embeds['embeds'] = df["embedding_128"]
print(data_embeds.head())
print(data_embeds.shape)

import umap # REDUCE DIMENSIONALITY
import seaborn as sns
sns.color_palette('colorblind')

X_umap = umap.UMAP().fit_transform(data_embeds['embeds'].tolist())

X_umap

data_df = pd.DataFrame()
data_df['x'] = X_umap[:,0]
data_df['y'] = X_umap[:, 1]
data_df['cluster'] = df['d2v_clusters']

data_df

data_df['clus'] = data_df['cluster'].apply(lambda x: str(x))
data_df

plt.figure(figsize=(7,7))
sns.scatterplot(data=data_df, x="x", y="y", hue='clus')
#plt.show()

"""What about PCA and Doc2Vec?"""

#Getting unique labels
pca = PCA(2)
#Transform the data
X_pca = pca.fit_transform(data_embeds['embeds'].tolist())
X_pca

data_df = pd.DataFrame()
data_df['x'] = X_pca[:,0]
data_df['y'] = X_pca[:, 1]
data_df['cluster'] = df['d2v_clusters']

data_df

data_df['clus'] = data_df['cluster'].apply(lambda x: str(x))
data_df

plt.figure(figsize=(7,7))
sns.scatterplot(data=data_df, x="x", y="y", hue='clus')

plt.title("Clusters of errors")
plt.savefig("./cluster_errors.pdf", bbox_inches='tight')
#plt.show()

"""### t-SNE

## Let's get error closest to the centroid in each cluster
"""

centroids = kmeans.cluster_centers_

len(centroids)

from sklearn.neighbors import NearestNeighbors

# Find closest to each centroid
closest_docs = []
for c, centroid in enumerate(centroids):
    nbrs = NearestNeighbors(n_neighbors=1).fit(df[df['d2v_clusters']==c]['embedding_128'].tolist())
    distances, indices = nbrs.kneighbors(centroid.reshape(1,-1))
    closest_doc = df[df['d2v_clusters']==c]['clean'].iloc[indices[0,0]]
    closest_doc_cat = df[df['d2v_clusters']==c]['Category'].iloc[indices[0,0]]
    closest_docs.append({c:[closest_doc, closest_doc_cat]})

closest_docs

"""### Checking the XML file

* {0: ['parameter <a_parameter_name> should be final', 'FinalParameters ']}, Miscellaneous other checks.
* {1: ['<operator> is not preceded with whitespace', 'WhitespaceAround ']}, Checks for whitespace.
* {2: ['line is longer than characters (found )', 'LineLength ']},Checks for Size Violations
* {3: ['<braket> is followed by whitespace', 'ParenPad ']},Checks for whitespace    
* {4: ['line has trailing spaces', 'RegexpSingleline ']},Miscellaneous other checks.
*  {5: ["name <a_name> must match pattern '^a-za-za-z-*$'", 'TypeName ']},Checks for Naming Conventions.
* {6: ['missing a javadoc comment', 'MissingJavadocMethod ']},Checks for Javadoc comments

## Let's try summarizing the whole cluster
"""

